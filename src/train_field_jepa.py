"""
train_field_jepa.py

Training script for the dynamic field + JEPA head.

Behavior:
    - Builds a DynamicField for data generation (on CPU).
    - Builds a StimulusDataset and DataLoader.
    - Builds a JEPAHead (on GPU/CPU).
    - Trains JEPAHead to predict u_{t+Î”} from (u_t, I_t) using MSE.
"""

from typing import Tuple

import torch
from torch.utils.data import DataLoader

from .configs import TRAIN_CONFIG
from .dynamic_field import DynamicField
from .stimulus_dataset import StimulusDataset
from .jepa_head import JEPAHead
from .utils.seed import set_seed


def train() -> None:
    """
    Train JEPAHead to predict future field states generated by DynamicField.
    """
    cfg = TRAIN_CONFIG

    device = torch.device(cfg.device if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Reproducibility
    set_seed(cfg.seed)

    # 1) Create a field instance for data generation (CPU)
    field_for_data = DynamicField(
        field_size=cfg.field_size,
        kernel_size=7,
        diffusion=cfg.diffusion,
        use_diffusion=cfg.use_diffusion,
    )

    # 2) Dataset & DataLoader (on CPU)
    dataset = StimulusDataset(
        field=field_for_data,
        num_sequences=cfg.num_sequences,
        seq_len=cfg.seq_len,
        field_size=cfg.field_size,
        device=torch.device("cpu"),
    )

    dataloader = DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=True,
        drop_last=True,
    )

    # 3) JEPA head model (on GPU if available)
    jepa_head = JEPAHead(
        hidden_channels=32,
        kernel_size=5,
    ).to(device)

    optimizer = torch.optim.Adam(jepa_head.parameters(), lr=cfg.learning_rate)
    loss_fn = torch.nn.MSELoss()

    jepa_head.train()

    for epoch in range(cfg.num_epochs):
        running_loss = 0.0
        num_batches = 0

        for batch_idx, (u_seq, I_seq) in enumerate(dataloader):
            # u_seq, I_seq: (B, T, 1, N) on CPU
            u_seq = u_seq.to(device)
            I_seq = I_seq.to(device)

            delta = cfg.delta
            T = u_seq.size(1)
            if T <= delta:
                continue  # not enough steps

            # context: t = 0 .. T-delta-1
            # target:  t = delta .. T-1
            u_ctx = u_seq[:, :-delta]   # (B, T_ctx, 1, N)
            I_ctx = I_seq[:, :-delta]   # (B, T_ctx, 1, N)
            u_tgt = u_seq[:, delta:]    # (B, T_ctx, 1, N)

            B, T_ctx, C, N = u_ctx.shape
            assert C == 1

            # Flatten time into batch dimension
            u_ctx_flat = u_ctx.reshape(B * T_ctx, C, N)
            I_ctx_flat = I_ctx.reshape(B * T_ctx, C, N)
            u_tgt_flat = u_tgt.reshape(B * T_ctx, C, N)

            # Predict future field
            u_pred_flat = jepa_head(u_ctx_flat, I_ctx_flat)

            loss = loss_fn(u_pred_flat, u_tgt_flat)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            num_batches += 1

        avg_loss = running_loss / max(1, num_batches)
        print(f"Epoch {epoch+1:03d} | loss = {avg_loss:.6f}")


if __name__ == "__main__":
    train()