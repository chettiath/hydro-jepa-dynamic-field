"""
train_field_jepa.py

Training script for the dynamic field + JEPA head.

Behavior:
    - Builds a DynamicField for data generation (on CPU).
    - Builds a StimulusDataset and DataLoader.
    - Builds a JEPAHead (on GPU/CPU).
    - Trains JEPAHead to predict u_{t+Δ} from (u_t, I_t) using MSE.
"""

import os
import random
from typing import Dict

import numpy as np
import torch
from torch.utils.data import DataLoader

from .configs import TRAIN_CONFIG
from .dynamic_field import DynamicField
from .stimulus_dataset import StimulusDataset
from .jepa_head import JEPAHead
from .utils.seed import set_seed


def select_device(device_pref: str = "auto") -> torch.device:
    """
    Choose a device given user preference with cuda → mps → cpu fallback.
    """
    if device_pref == "auto":
        if torch.cuda.is_available():
            device_str = "cuda"
        elif torch.backends.mps.is_available():
            device_str = "mps"
        else:
            device_str = "cpu"
    else:
        device_str = device_pref

    try:
        device = torch.device(device_str)
    except Exception:
        device = torch.device("cpu")
        print(f"Warning: requested device '{device_str}' unavailable; falling back to CPU.")
        return device

    if device.type == "cuda" and not torch.cuda.is_available():
        print("Warning: CUDA requested but not available; falling back to CPU.")
        device = torch.device("cpu")
    if device.type == "mps" and not torch.backends.mps.is_available():
        print("Warning: MPS requested but not available; falling back to CPU.")
        device = torch.device("cpu")
    return device


def train() -> None:
    """
    Train JEPAHead to predict future field states generated by DynamicField.
    """
    cfg = TRAIN_CONFIG
    if cfg.seq_len <= cfg.delta:
        raise ValueError(
            f"seq_len ({cfg.seq_len}) must be greater than delta ({cfg.delta}) for training."
        )

    device = select_device(cfg.device)
    print(f"Using device: {device}")

    # Reproducibility
    set_seed(cfg.seed)

    # 1) Create a field instance for data generation (CPU)
    field_for_data = DynamicField(
        field_size=cfg.field_size,
        kernel_size=cfg.kernel_size,
        diffusion=cfg.diffusion,
        use_diffusion=cfg.use_diffusion,
        leak=cfg.leak,
        use_mexican_hat_init=cfg.use_mexican_hat_init,
    )

    # 2) Dataset & DataLoader (on CPU)
    dataset = StimulusDataset(
        field=field_for_data,
        num_sequences=cfg.num_sequences,
        seq_len=cfg.seq_len,
        field_size=cfg.field_size,
        device=torch.device("cpu"),
    )

    def _seed_worker(worker_id: int) -> None:
        worker_seed = cfg.seed + worker_id
        np.random.seed(worker_seed)
        random.seed(worker_seed)
        torch.manual_seed(worker_seed)

    dataloader = DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=0,  # keep deterministic on-the-fly generation
        worker_init_fn=_seed_worker,
    )

    # 3) JEPA head model (on GPU if available)
    jepa_kwargs: Dict[str, int] = {
        "hidden_channels": cfg.jepa_hidden_channels,
        "kernel_size": cfg.jepa_kernel_size,
    }
    jepa_head = JEPAHead(**jepa_kwargs).to(device)

    optimizer = torch.optim.Adam(jepa_head.parameters(), lr=cfg.learning_rate)
    loss_fn = torch.nn.MSELoss()

    jepa_head.train()

    for epoch in range(cfg.num_epochs):
        running_loss = 0.0
        num_batches = 0

        for batch_idx, (u_seq, I_seq) in enumerate(dataloader):
            # u_seq, I_seq: (B, T, 1, N) on CPU
            u_seq = u_seq.to(device)
            I_seq = I_seq.to(device)

            delta = cfg.delta
            T = u_seq.size(1)
            if T <= delta:
                continue  # not enough steps

            # context: t = 0 .. T-delta-1
            # target:  t = delta .. T-1
            u_ctx = u_seq[:, :-delta]   # (B, T_ctx, 1, N)
            I_ctx = I_seq[:, :-delta]   # (B, T_ctx, 1, N)
            u_tgt = u_seq[:, delta:]    # (B, T_ctx, 1, N)

            B, T_ctx, C, N = u_ctx.shape
            assert C == 1

            # Flatten time into batch dimension
            u_ctx_flat = u_ctx.reshape(B * T_ctx, C, N)
            I_ctx_flat = I_ctx.reshape(B * T_ctx, C, N)
            u_tgt_flat = u_tgt.reshape(B * T_ctx, C, N)

            # Predict future field
            u_pred_flat = jepa_head(u_ctx_flat, I_ctx_flat)

            loss = loss_fn(u_pred_flat, u_tgt_flat)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            num_batches += 1

        avg_loss = running_loss / max(1, num_batches)
        print(f"Epoch {epoch+1:03d} | loss = {avg_loss:.6f}")

    # Save checkpoint
    ckpt_dir = "checkpoints"
    os.makedirs(ckpt_dir, exist_ok=True)
    ckpt_path = os.path.join(ckpt_dir, "jepa_head.pth")
    torch.save(
        {
            "model_state": jepa_head.state_dict(),
            "config": cfg.__dict__,
            "model_kwargs": jepa_kwargs,
            "data_gen_kwargs": {
                "field_size": cfg.field_size,
                "kernel_size": cfg.kernel_size,
                "diffusion": cfg.diffusion,
                "use_diffusion": cfg.use_diffusion,
                "leak": cfg.leak,
                "use_mexican_hat_init": cfg.use_mexican_hat_init,
                "seq_len": cfg.seq_len,
                "delta": cfg.delta,
            },
        },
        ckpt_path,
    )
    print(f"Saved checkpoint to: {ckpt_path}")


if __name__ == "__main__":
    train()
